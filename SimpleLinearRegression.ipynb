{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d7ba1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple linear regression\n",
    "#Importing libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "#Viewing how linear a relationship is\n",
    "plt.scatter(cdf.FUELCONSUMPTION, cdf.CO2EMISSIONS, color='blue')\n",
    "plt.xlabel(\"FUEL CONSUMPTION\")\n",
    "plt.ylabel(\"Emission\")\n",
    "plt.show()\n",
    "\n",
    "#Creating a train and test dataset\n",
    "msk = np.random.rand(len(df)) < 0.8\n",
    "train = cdf[msk]\n",
    "test = cdf[~msk]\n",
    "\n",
    "#Simple regression model\n",
    "from sklearn import linear_model\n",
    "regr = linear_model.LinearRegression()\n",
    "train_x = np.asanyarray(train[['ENGINE SIZE']])\n",
    "train_y = np.asanyarray(train[['CO2EMISSIONS']])\n",
    "regr.fit(train_x, train_y)\n",
    "print('Coefficients: ', regr.coef_)\n",
    "print('Intercept: ', regr.intercept_)\n",
    "\n",
    "#Plot the fit line over the data\n",
    "plt.scatter(train.ENGINESIZE, train.CO2EMISSIONS, color='blue')\n",
    "plt.plot(train_x, regr.coef_[0][0]*train_x + regr.intercept_[0], '-r')\n",
    "plt.xlabel(\"Engine size\")\n",
    "plt.ylabel(\"Emission\")\n",
    "\n",
    "#Mean Absolute error: It is the mean of the absolute value of the errors. This is the easiest of the metrics to understand since it’s just average error.\n",
    "#Mean Squared Error (MSE): Mean Squared Error (MSE) is the mean of the squared error. It’s more popular than Mean Absolute Error because the focus is geared more towards large errors. This is due to the squared term exponentially increasing larger errors in comparison to smaller ones.\n",
    "#Root Mean Squared Error (RMSE): the square root of the MSE.\n",
    "#R-squared is not an error, but rather a popular metric to measure the performance of your regression model. It represents how close the data points are to the fitted regression line. The higher the R-squared value, the better the model fits your data. The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse).\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "test_x = np.asanyarray(test[['ENGINESIZE']])\n",
    "test_y = np.asanyarray(test[['CO2EMISSIONS']])\n",
    "test_y_ = regr.predict(test_x)\n",
    "print(\"Mean absolute error: %.2f\" % np.mean(np.absolute(test_y_ - test_y)))\n",
    "print(\"Residual sum of squares (MSE): %.2f\" % np.mean((test_y_ - test_y) ** 2))\n",
    "print(\"R2-score: %.2f\" % r2_score(test_y , test_y_) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
