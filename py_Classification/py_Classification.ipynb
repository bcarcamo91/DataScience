{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f818ebc-9fc0-4a1b-b781-5469c2ef2a05",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c951c18f-f604-4c02-8c79-8f2f3cd83eb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "Two main types of classification problems:\n",
    "- Binary: exactly two classes to choose between (usually 0 and 1, true and false, or positive and negative)\n",
    "- Multiclass or multinomial classification: three or more classes of the outputs to choose from"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a51bf2-2bc0-48dc-aa48-ccadb2d3e512",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7157544-7b35-4313-a750-370d0b90c4ba",
   "metadata": {},
   "source": [
    "Logistic regression is a fundamental classification technique. It belongs to the group of linear classifiers and is somewhat similar to polynomial and linear regression. Logistic regression is fast and relatively uncomplicated, and it‚Äôs convenient for you to interpret the results. Although it‚Äôs essentially a method for binary classification, it can also be applied to multiclass problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d7987e-ce98-428a-bf8b-2b5f57228bff",
   "metadata": {},
   "source": [
    "<img src=\"https://files.realpython.com/media/log-reg-1.e32deaa7cbac.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d209cbf-ffb8-41f8-9b5d-032481549169",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3c55bc-297c-455f-ad9b-49c0c2e1fc35",
   "metadata": {},
   "source": [
    "Logistic regression is a linear classifier, so you‚Äôll use a linear function ùëì(<b>ùê±</b>) = ùëè‚ÇÄ + ùëè‚ÇÅùë•‚ÇÅ + ‚ãØ + ùëè·µ£ùë•·µ£, also called the <u>logit</u>. The variables ùëè‚ÇÄ, ùëè‚ÇÅ, ‚Ä¶, ùëè·µ£ are the estimators of the regression coefficients, which are also called the predicted weights or just coefficients.\n",
    "\n",
    "The logistic regression function ùëù(ùê±) is the sigmoid function of ùëì(ùê±): ùëù(ùê±) = 1 / (1 + exp(‚àíùëì(ùê±)). As such, it‚Äôs often close to either 0 or 1. The function ùëù(ùê±) is often interpreted as the predicted probability that the output for a given ùê± is equal to 1. Therefore, 1 ‚àí ùëù(ùë•) is the probability that the output is 0.\n",
    "\n",
    "Logistic regression determines the best predicted weights ùëè‚ÇÄ, ùëè‚ÇÅ, ‚Ä¶, ùëè·µ£ such that the function ùëù(ùê±) is as close as possible to all actual responses ùë¶·µ¢, ùëñ = 1, ‚Ä¶, ùëõ, where ùëõ is the number of observations. The process of calculating the best weights using available observations is called model training or fitting.\n",
    "\n",
    "To get the best weights, you usually maximize the log-likelihood function (LLF) for all observations ùëñ = 1, ‚Ä¶, ùëõ. This method is called the maximum likelihood estimation and is represented by the equation LLF = Œ£·µ¢(ùë¶·µ¢ log(ùëù(ùê±·µ¢)) + (1 ‚àí ùë¶·µ¢) log(1 ‚àí ùëù(ùê±·µ¢))).\n",
    "\n",
    "When ùë¶·µ¢ = 0, the LLF for the corresponding observation is equal to log(1 ‚àí ùëù(ùê±·µ¢)). If ùëù(ùê±·µ¢) is close to ùë¶·µ¢ = 0, then log(1 ‚àí ùëù(ùê±·µ¢)) is close to 0. This is the result you want. If ùëù(ùê±·µ¢) is far from 0, then log(1 ‚àí ùëù(ùê±·µ¢)) drops significantly. You don‚Äôt want that result because your goal is to obtain the maximum LLF. Similarly, when ùë¶·µ¢ = 1, the LLF for that observation is ùë¶·µ¢ log(ùëù(ùê±·µ¢)). If ùëù(ùê±·µ¢) is close to ùë¶·µ¢ = 1, then log(ùëù(ùê±·µ¢)) is close to 0. If ùëù(ùê±·µ¢) is far from 1, then log(ùëù(ùê±·µ¢)) is a large negative number.\n",
    "\n",
    "There are several mathematical approaches that will calculate the best weights that correspond to the maximum LLF, but that‚Äôs beyond the scope of this tutorial. For now, you can leave these details to the logistic regression Python libraries you‚Äôll learn to use here!\n",
    "\n",
    "Once you determine the best weights that define the function ùëù(ùê±), you can get the predicted outputs ùëù(ùê±·µ¢) for any given input ùê±·µ¢. For each observation ùëñ = 1, ‚Ä¶, ùëõ, the predicted output is 1 if ùëù(ùê±·µ¢) > 0.5 and 0 otherwise. The threshold doesn‚Äôt have to be 0.5, but it usually is. You might define a lower or higher value if that‚Äôs more convenient for your situation.\n",
    "\n",
    "There‚Äôs one more important relationship between ùëù(ùê±) and ùëì(ùê±), which is that log(ùëù(ùê±) / (1 ‚àí ùëù(ùê±))) = ùëì(ùê±). This equality explains why ùëì(ùê±) is the logit. It implies that ùëù(ùê±) = 0.5 when ùëì(ùê±) = 0 and that the predicted output is 1 if ùëì(ùê±) > 0 and 0 otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005ee232-ea58-4640-a682-afea10d811bc",
   "metadata": {},
   "source": [
    "## Classification Performance\n",
    "Binary classification has four possible types of results:\n",
    "- True negatives: correctly predicted negatives (zeros)\n",
    "- True positives: correctly predicted positives (ones)\n",
    "- False negatives: incorrectly predicted negatives (zeroes)\n",
    "- False positives: incorrectly predicted positives (ones)\n",
    "\n",
    "The most straightforward indicator of <b>classification accuracy</b> is the ratio of the number of correct predictions to the total numbr of predictions (or oservations). Other indicators of binary classifiers include the following:\n",
    "- The <b>positive prdictive value</b>: the ratio of the number of true positives to the sum of the numbers of true and false positives\n",
    "- The <b>negatgive preictive value</b>: the ratio of the number of true negatives to the sum of the numbers of true and false negatives\n",
    "- The <b>sensitivity</b> (also known as <b>recall</b> or <b>true positive rate</b>): the ratio of the number of true positive to the number of actual positives\n",
    "- The <b>specificity</b> (or true negative rate): the ratio of the number of true negatives tot he number of actual negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f828bea-8204-4888-b5f8-45aacaa0f565",
   "metadata": {},
   "source": [
    "## Single-Variate Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12804a96-903f-4bdb-80f6-0bbebee63c4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Single-variate logistic regression is the most straightforward case of logistic regression. There is only one independent variable (or feature), which is ùê± = ùë•. This figure illustrates single-variate logistic regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ea460b-88bb-4bc4-8404-314b8f085894",
   "metadata": {},
   "source": [
    "<img src=\"https://files.realpython.com/media/log-reg-2.e88a21607ba3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d8b6ac-ec2b-48ce-b5ab-0656f7cb1d68",
   "metadata": {},
   "source": [
    "## Multi-Variate Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41202988-12e3-49c8-a277-e3c6169a53a5",
   "metadata": {},
   "source": [
    "Multi-variate logistic regression has more than one input variable. This figure shows the classification with two independent variables, ùë•‚ÇÅ and ùë•‚ÇÇ:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1bcbfb-f759-4474-98d4-056ee8cd4ec0",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"https://files.realpython.com/media/log-reg-3.b1634d335c4f.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4256d4-3b1c-4761-a9ee-77917206b28d",
   "metadata": {},
   "source": [
    "Logistic regression determines the weights ùëè‚ÇÄ, ùëè‚ÇÅ, and ùëè‚ÇÇ that maximize the LLF. Once you have ùëè‚ÇÄ, ùëè‚ÇÅ, and ùëè‚ÇÇ, you can get:\n",
    "\n",
    "- The logit ùëì(ùë•‚ÇÅ, ùë•‚ÇÇ) = ùëè‚ÇÄ + ùëè‚ÇÅùë•‚ÇÅ + ùëè‚ÇÇùë•‚ÇÇ\n",
    "- The probabilities ùëù(ùë•‚ÇÅ, ùë•‚ÇÇ) = 1 / (1 + exp(‚àíùëì(ùë•‚ÇÅ, ùë•‚ÇÇ)))\n",
    "\n",
    "The dash-dotted black line linearly separates the two classes. This line corresponds to ùëù(ùë•‚ÇÅ, ùë•‚ÇÇ) = 0.5 and ùëì(ùë•‚ÇÅ, ùë•‚ÇÇ) = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a179138c-dec3-4bab-9783-9278712929ad",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631b1812-58f1-4ba0-a25f-22eae6f29429",
   "metadata": {
    "tags": []
   },
   "source": [
    "Overfitting is one of the most serious kinds of problems related to machine learning. It occurs when a model learns the training data too well. The model then learns not only the relationships among data but also the noise in the dataset. Overfitted models tend to have good performance with the data used to fit them (the training data), but they behave poorly with unseen data (or test data, which is data not used to fit the model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a00825-3f5c-4ddf-92cf-19178ec798c6",
   "metadata": {},
   "source": [
    "<b><u>Regularization</u></b> normally tries to reduce or penalize the complexity of the model. Regularization techniques applied with logistic regression mostly tend to penalize large coefficients ùëè‚ÇÄ, ùëè‚ÇÅ, ‚Ä¶, ùëè·µ£:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbe1c03-86e1-45a6-bb17-09317465ad5a",
   "metadata": {},
   "source": [
    "- L1 regularization penalizes the LLF with the scaled sum of the absolute values of the weights: |ùëè‚ÇÄ|+|ùëè‚ÇÅ|+‚ãØ+|ùëè·µ£|.\n",
    "- L2 regularization penalizes the LLF with the scaled sum of the squares of the weights: ùëè‚ÇÄ¬≤+ùëè‚ÇÅ¬≤+‚ãØ+ùëè·µ£¬≤.\n",
    "- Elastic-net regularization is a linear combination of L1 and L2 regularization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38460d0-2a3a-47ff-bcb6-54845726c86a",
   "metadata": {},
   "source": [
    "Step 1: Import Packages, Functions, and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfa3441-5281-404a-91dd-cc642c0c73ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a49e6fa-639a-4c58-9a09-c94124c11d26",
   "metadata": {},
   "source": [
    "Step 2: Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02287978-305d-4e78-ae67-77240f505c2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = np.arange(10).reshape(-1, 1)\n",
    "y = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98d282f-2a04-4634-9d8f-8aff68bf1918",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f9f710-6937-4eef-b840-3819cf2522da",
   "metadata": {},
   "source": [
    "Step 3: Create a Model and Train It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719518ed-34a2-4756-be76-6c6306f926a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver='liblinear', random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c752d0fd-3640-4873-809b-14ae556a81da",
   "metadata": {},
   "source": [
    "Optional parameters of LogisticRegression class\n",
    "- <b>penalty</b>: a string ('12' by default) that decides whether there is regularization and which approach to use. other options are 'l1', 'elasticnet', and 'none'\n",
    "- <b>dual</b>: a boolean (false by default) that decides whether to use primal (when false) or dual formulation (when rue)\n",
    "- <b>tol</b>: is a floatingp-point number( 0.0001 by default) that defines the toleranc for stopping the procdurfe\n",
    "- <b>c</b>: is a positive floating-point number (1.0 by default) that defines the relative strength of regularization. Smaller values indicate stronger regularizatgion.\n",
    "- <b>fit_injtercept</b>: a Boolean (True by default) that decies whether to calculate the itnerecpt b0 (when True) or consider it equal to zero (when False) \n",
    "- <b>intercept_scaling</b>: is a floating-point number (1.0 by default) that defines the scaling of the intecept b0. \n",
    "- <b>class_weight</b>: a dictionary, 'balanced', or None(default) that defines the weights related to each class. When None, all classes have the weight one.\n",
    "- <b>random_state</b>: is an integer, an instanec of numpy.Randomstate, or None (defaujlt) that dfeines what pseudo-random number genrator to use.\n",
    "- <b>solver</b>: a string ('liblinear' by default) that decides what solve rto use for fitin the moel. Other options are 'newton-cg', 'lbfgs', 'sag', and 'saga'.\n",
    "- <b>max-iter</b>: an integer (100 by default) tha defines the maximum number of iterations by the solver during model fitting\n",
    "- <b>multi-class</b>: a string ('ovr' by default) that decies the approach to use for handling multiple classes. Other options are 'multinomial' and 'auto'. \n",
    "- <b>verbose</b>: a non-negative integer (0 by default) that defines the verbosity for the 'liblinear' and 'lbfgs' solvers\n",
    "- <b>warm_start</b>:a Boolean (False by default) that decis whether to reuse the previously obtained solution\n",
    "- <b>n_jobs</b>: an integer or None (default) that definest he number of paralle,l processes to use. None usually means to use one core, while -1 mean sto use all available cores\n",
    "- <b>l1_ratio</b>: a floating-point number between zero and one or None (default). It defies the relative importance of the L1 part int he elastic-net regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4597c0-c4df-4ddc-b3dd-13066ed71716",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver='liblinear', random_state=0).fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9d651b-6397-4597-9dc9-b4b4bc51f234",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Represents the array of disctinct values that y takes.\n",
    "print(model.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dc44da-d9d5-40aa-b9f7-a5aa3248378a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.intercept_, model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cceab637-e9f6-4dc0-a409-268d9b4945d1",
   "metadata": {},
   "source": [
    "Step 4: Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37941a9c-4589-483b-b1ed-28725d712550",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.predict_proba(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c194cb6b-c975-457d-9933-7abceb8cb855",
   "metadata": {},
   "source": [
    "In the matrix above, each row corresponds to a single observation. The first column is the probability of the predicted output being zero, that is 1 - ùëù(ùë•). The second column is the probability that the output is one, or ùëù(ùë•).\n",
    "\n",
    "You can get the actual predictions, based on the probability matrix and the values of ùëù(ùë•), with .predict():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8e559d-6c69-4192-b737-aa8decdaacb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3021c2f5-a9c1-49e5-b2c5-54d0e5e8cc38",
   "metadata": {},
   "source": [
    "<img src=\"https://files.realpython.com/media/log-reg-5.1e0f3f7e733a.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77b8513-37c7-47c1-b752-fa40d7e54cda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Obtain the accuracy of your model with .score(); this is the ratio of the number of correct predictions tot he number of observations\n",
    "model.score(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f35a01c-d74d-4c6a-bde0-93964fac3ca9",
   "metadata": {},
   "source": [
    "You can get more information on the accuracy of the model with a confusion matrix. In the case of binary classificaiton, the confusion matrix showst he numbers of the following: \n",
    "- True negatives in the upper-left position\n",
    "- False negatives in the lower-left position\n",
    "- False positives in the upper-right position\n",
    "- True positives int he lower-right position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7c7869-768f-43bf-9235-157b7f50f3e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create confusion matrix\n",
    "confusion_matrix(y, model.predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70892c7e-caa3-4e51-9b48-3be714c7cd86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Visualize the confusion matrix:\n",
    "cm = confusion_matrix(y, model.predict(x))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.imshow(cm)\n",
    "ax.grid(False)\n",
    "ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))\n",
    "ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))\n",
    "ax.set_ylim(1.5, -0.5)\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        ax.text(j, i, cm[i, j], ha='center', va='center', color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb71c0c4-b850-43c7-bcc1-c75c3aa9271a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Get a more comprehensive report on the classification with classification_report()\n",
    "print(classification_report(y, model.predict(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba59c13-2a82-41b0-ae35-f718cab437a9",
   "metadata": {},
   "source": [
    "## Improve the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78658dec-84d1-4d3d-a194-9a757dc3013b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#One way is wyb changing the regularization strength c equal to 10.0\n",
    "model = LogisticRegression(solver='liblinear', C=10.0, random_state=0)\n",
    "model.fit(x, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48868eb1-92b8-45f3-98a2-0af7d30a74f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This model will have different parameters, a differen probability matrix and a different set of coefficients and predictions\n",
    "model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a278bf-6da1-4894-8df8-03d3f6ab7300",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02d16d4-7d27-4fab-991f-4f4779d2002c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.predict_proba(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610a3d3e-bd51-479c-9f53-37106d3abb2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aaa2aa-ec99-48fd-9f90-0ff828f9728f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.score(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d857e1-005a-464e-a4c2-211d18c759d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "confusion_matrix(y, model.predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02dd93b-2b34-44dd-812d-ff0278eb3b2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(classification_report(y, model.predict(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ed38a8-dfba-48ed-993f-784d446cfeca",
   "metadata": {},
   "source": [
    "<img src=\"https://files.realpython.com/media/log-reg-7.9141027bd736.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f0a2e5-958b-449a-b21d-db8dd730f031",
   "metadata": {},
   "source": [
    "## Logistic Regression with sci-kit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76916d0-eaba-4d99-be9a-4eeb270d5ce8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Import packages, functions, and classes\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Step 2: Get data\n",
    "x = np.arange(10).reshape(-1, 1)\n",
    "y = np.array([0, 1, 0, 0, 1, 1, 1, 1, 1, 1])\n",
    "\n",
    "# Step 3: Create a model and train it\n",
    "model = LogisticRegression(solver='liblinear', C=10.0, random_state=0)\n",
    "model.fit(x, y)\n",
    "\n",
    "# Step 4: Evaluate the model\n",
    "p_pred = model.predict_proba(x)\n",
    "y_pred = model.predict(x)\n",
    "score_ = model.score(x, y)\n",
    "conf_m = confusion_matrix(y, y_pred)\n",
    "report = classification_report(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4412d6eb-1fb6-496a-904f-62bcc2684e32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('x:', x , sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36428b8c-9272-4f22-8e50-d4e1543b7813",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('y:', y, sep='\\n', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6001f12b-597a-4f8b-bd15-b5ecc85b05ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('intercept:', model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9b4cfe-80b9-41cd-a7c7-47e52a750b3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('coef:', model.coef_, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a9c708-fb7f-4d83-ae23-e01431231c34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('p_pred', p_pred, sep='\\n', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3cad83-8dbd-4aa2-a185-058c518abeb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('y_pred:', y_pred, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305a8cff-c573-4686-9354-cb871d56cd5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('score_:', score_, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8e7c0f-cde5-4237-87bf-4e439c08a80a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('conf_m:', conf_m, sep='\\n', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06d1a85-4614-48fe-ad9d-456155791dd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('report:', report, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672b7b2b-d260-4b28-9d3c-3651806c3f43",
   "metadata": {},
   "source": [
    "## Logistic Regression in Python with Statsmodels Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c31507-e859-4897-8ce3-5f8da71d3972",
   "metadata": {},
   "source": [
    "Step 1: Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc31c903-4375-4248-8534-230eff864e02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e238936-6e7c-4133-95ab-6b7779db9c8f",
   "metadata": {},
   "source": [
    "Step 2: Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913342b9-e32c-434a-90c0-07e3b59853e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = np.arange(10).reshape(-1, 1)\n",
    "y = np.array([0, 1, 0, 0, 1, 1, 1, 1, 1, 1])\n",
    "x = sm.add_constant(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525b0666-9a4c-41ff-a3a7-37803c074cef",
   "metadata": {
    "tags": []
   },
   "source": [
    "add_constant() takes the array x as the argument and returns a new array witht he additional column of ones. This is how x and y look."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542f7a23-c193-4303-ac0d-cd9b3a586dfb",
   "metadata": {},
   "source": [
    "Step 3: Create a Model and Train It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7df9fd5-5d1b-4361-8d59-e1e31d90973f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.Logit(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f908b112-d60f-4cc5-b85f-e693f77f1365",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = model.fit(method='newton')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d1b829-66b0-431e-8400-f51167360dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcad59b-13f1-4a93-99c9-fbd6ac57f738",
   "metadata": {},
   "source": [
    "Step 4: Evaluate the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f81648-da66-4b81-96d2-6c8e428f5a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.predict(x)\n",
    "(result.predict(x) >= 0.5).astype(int)\n",
    "\n",
    "result.pred_table()\n",
    "result.summary()\n",
    "result.summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa87c463-243d-4365-9d26-e1996ff4cd1e",
   "metadata": {},
   "source": [
    "## Logistic Regression Example with Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4c5ebe-8ae6-49f4-887b-564f3e48f066",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic regression\n",
    "import pandas as pd\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22980fb7-4d9f-4e6f-9e46-894713b474be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataset\n",
    "await download (path, \"ChurnData.csv\")\n",
    "path = \"ChurnData.csv\"\n",
    "churn_df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cf22f6-0b7e-484c-80c9-9c343748f645",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-processing\n",
    "churn_df = churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip',   'callcard', 'wireless','churn']]\n",
    "churn_df['churn'] = churn_df['churn'].astype('int') #int required\n",
    "churn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d7a5ee-b338-4eb3-8e0c-d1e244e1f875",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define x and y\n",
    "X = np.asarray(churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip']])\n",
    "y = np.asarray(churn_df['churn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d131c98a-078e-4fa1-9a1d-9b11b173c11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing\n",
    "from sklearn import preprocessing\n",
    "x = preprocessing.StandardScaler().fit(X).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc7da71-bd09-45c7-b5a8-bce9687e91da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbb4ac8-b4d4-402c-9bda-e4226f7f0995",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model building\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "LR = LogisticRegression(C = 0.01, solver=\"liblinear\").fit(X_train, y_train)\n",
    "LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c78af8-327e-48d8-9272-664886f0ee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predictng\n",
    "yhat = LR.predict(X_test)\n",
    "yhat_prob = LR.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1086b63-a906-4c3d-9912-f735201aa6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jaccard index for accuracy evaluation\n",
    "from sklearn.metrics import jaccard_score\n",
    "jaccard_score(y_test, yhat, pos_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23768d2-c0ff-4a02-a4e1-c73958a85604",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                            normalize=False,\n",
    "                            title='Confusion matrix',\n",
    "                              cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "            print('Confusion matrix, without normalization')\n",
    "            \n",
    "        print(cm)\n",
    "        \n",
    "        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "        plt.title(title)\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arrange(len(classes))\n",
    "        plt.xticks(tick_marks, classes, rotation=45)\n",
    "        plt.yticks(tick_marks, classes)\n",
    "        \n",
    "        fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "print(confusion_matrix(y_test, yhat, labels=[1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a324e269-5812-46c6-9031-e2d0652e983e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, yhat, labels=[1, 0])\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "#Plot non-normalized confusion matrix\n",
    "plt.figure(\n",
    "plot_confusion_matrix(cnf_matrix, classes=['churn=1', 'churn'=0], normalize= False,  title='Confusion matrix')\n",
    "    \n",
    "#Precision: TP / (TP + FP)\n",
    "#Recall: TP / (TP + FN)\n",
    "    \n",
    "#Log loss\n",
    "from sklearn.metrics import log_loss\n",
    "    log_loss(y_test, yhat_prob)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
